{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query PDF Vector Database\n",
    "# This notebook retrieves relevant passages from the PDF vector database based on user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved index\n",
    "index_path = \"faiss_combined_index\"  # Path to your saved index\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_pdf(query: str, k: int = 5):\n",
    "    \"\"\"Search for relevant passages in the PDF.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query\n",
    "        k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        list: List of relevant text passages\n",
    "    \"\"\"\n",
    "    results = vector_store.search(query, k=k)\n",
    "    return [result['text'] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant passages:\n",
      "\n",
      "Passage 1:\n",
      "I Fundamental Algorithms for NLP: Some of the bigram probabilities above encode some facts that we think of as strictly\n",
      "syntactic in nature, like the fact that what comes after eatis usually a noun or an\n",
      "adjective, or that what comes after tois usually a verb. Others might be a fact about\n",
      "the personal assistant task, like the high probability of sentences beginning with\n",
      "the words I. And some might even be cultural rather than linguistic, like the higher\n",
      "probability that people are looking for Chinese versus English food.\n",
      "\n",
      "Passage 2:\n",
      "I Fundamental Algorithms for NLP: P(ﬁshjThanks for all the )\n",
      "Language models give us the ability to assign such a conditional probability to every\n",
      "possible next word, giving us a distribution over the entire vocabulary. We can also\n",
      "assign probabilities to entire sequences by combining these conditional probabilities\n",
      "with the chain rule:\n",
      "P(w1:n) =nY\n",
      "i=1P(wijw<i)\n",
      "The n-gram language models of Chapter 3 compute the probability of a word given\n",
      "counts of its occurrence with the n\u00001 prior words. The context is thus of size n\u00001.\n",
      "\n",
      "Passage 3:\n",
      "III Annotating Linguistic Structure: with one noun (argument).\n",
      "The conditional probability model can be computed by parsing a very large cor-\n",
      "pus (billions of words), and computing co-occurrence counts: how often a given\n",
      "verb occurs with a given noun in a given relation. The conditional probability of an\n",
      "argument noun given a verb for a particular relation P(njv;r)can then be used as a\n",
      "selectional preference metric for that pair of words (Brockmann and Lapata 2003,\n",
      "Keller and Lapata 2003):\n",
      "P(njv;r) =(\n",
      "C(n;v;r)\n",
      "C(v;r)ifC(n;v;r)>0\n",
      "\n",
      "Passage 4:\n",
      "I Fundamental Algorithms for NLP: word. For example, if the preceding context is “Thanks for all the” and we want to\n",
      "know how likely the next word is “ﬁsh” we would compute:\n",
      "P(ﬁshjThanks for all the )\n",
      "Language models give us the ability to assign such a conditional probability to every\n",
      "possible next word, giving us a distribution over the entire vocabulary. The n-gram9.5 • T HELANGUAGE MODELING HEAD 199\n",
      "language models of Chapter 3 compute the probability of a word given counts of\n",
      "\n",
      "Passage 5:\n",
      "I Fundamental Algorithms for NLP: ways to estimate the probability of a word wgiven a history h, or the probability of\n",
      "an entire word sequence W.\n",
      "Let’s start with some notation. First, throughout this chapter we’ll continue to\n",
      "refer to words , although in practice we usually compute language models over to-\n",
      "kens like the BPE tokens of page 21. To represent the probability of a particular\n",
      "random variable Xitaking on the value “the”, or P(Xi=“the”), we will use the\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"How many types of probability are there in natural language processing?\"  # Your query here\n",
    "docs = vector_store.similarity_search(query, k=5)\n",
    "\n",
    "print(\"Relevant passages:\")\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nPassage {i}:\")\n",
    "    print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
